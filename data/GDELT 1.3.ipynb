{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1f8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43dc8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "THEME = 'politic'\n",
    "WESTERN_COUNTRY_CODES = ['US', 'CA', 'GB', 'UK', 'IE', 'FR',\n",
    "                         'DE', 'GM', 'NL', 'BE', 'LU', 'CH', 'AT', 'DK', 'SE', 'NO', 'FI',\n",
    "                        'IT', 'ES', 'PT', 'IS', 'AU', 'NZ', 'GR', 'CY', 'IL']\n",
    "MAJOR_SOURCES = ['cnn.com', 'theguardian.com', 'washingtonexaminer.com', 'newsweek.com', 'breitbart.com']\n",
    "ARTICLES_PER_SOURCE = 5\n",
    "\n",
    "# GDELT column indices\n",
    "# GDELT column indices\n",
    "DATE_COL = 0\n",
    "THEMES_COL = 3\n",
    "LOCATIONS_COL = 4\n",
    "TONE_COL = 7\n",
    "SOURCE_COL = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7304f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdelt_file_list():\n",
    "    \"\"\"Scrape the GDELT index page to get all available CSV files.\"\"\"\n",
    "    index_url = \"http://data.gdeltproject.org/gkg/index.html\"\n",
    "    try:\n",
    "        response = requests.get(index_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links ending with .gkg.csv.zip\n",
    "        file_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            # Match pattern: digits.gkg.csv.zip\n",
    "            if re.match(r'^\\d+\\.gkg\\.csv\\.zip$', href):\n",
    "                # Extract the date number\n",
    "                date_str = href.split('.')[0]\n",
    "                # Check if date is between 20151108 and 20251108\n",
    "                if len(date_str) == 8 and 20150101 <= int(date_str) <= 20251109:\n",
    "                    file_links.append(f\"http://data.gdeltproject.org/gkg/{href}\")\n",
    "        \n",
    "        return sorted(file_links, reverse=True)  # Most recent first\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching file list: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1776f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_parse_gdelt(file_url):\n",
    "    \"\"\"Download and parse a GDELT GKG file.\"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        response = requests.get(file_url, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            inner_filename = z.namelist()[0]\n",
    "            with z.open(inner_filename) as f:\n",
    "                df = pd.read_csv(f, sep=\"\\t\", header=None, low_memory=False)\n",
    "        \n",
    "        print(f\"Loaded {len(df):,} rows\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading/parsing {file_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8401f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_gdelt_data(df, theme, country_codes, sources):\n",
    "    \"\"\"Filter GDELT data by theme, location, and source.\"\"\"\n",
    "    # Filter by theme\n",
    "    theme_filter = df[THEMES_COL].astype(str).str.contains(theme, case=False, na=False)\n",
    "    \n",
    "    # Filter by Western countries\n",
    "    western_pattern = '|'.join([f'#{code}#' for code in country_codes])\n",
    "    location_filter = df[LOCATIONS_COL].astype(str).str.contains(western_pattern, case=False, na=False)\n",
    "    \n",
    "    # Filter by major sources\n",
    "    df[SOURCE_COL] = df[SOURCE_COL].astype(str).str.lower()\n",
    "    source_filter = df[SOURCE_COL].apply(lambda s: any(src in s for src in sources))\n",
    "    \n",
    "    # Combine filters\n",
    "    filtered_df = df[theme_filter & location_filter & source_filter]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c21d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_by_source(df, sources):\n",
    "    \"\"\"Extract URLs grouped by news source.\"\"\"\n",
    "    urls_by_source = {source: [] for source in sources}\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        source_urls_col = df.shape[1] - 1\n",
    "        raw_urls = str(row.iloc[source_urls_col])\n",
    "        source_name = str(row.iloc[SOURCE_COL]).lower()\n",
    "        date = str(row.iloc[DATE_COL])\n",
    "        tone = str(row.iloc[TONE_COL])\n",
    "        \n",
    "        # Split multiple URLs\n",
    "        urls = [u.strip() for u in raw_urls.split(';') if u.strip()]\n",
    "        \n",
    "        # Debug: print first few rows\n",
    "        if idx < 3:\n",
    "            print(f\"Debug - Row {idx}: source='{source_name}', urls={urls[:2]}\")\n",
    "        \n",
    "        # Categorize by source\n",
    "        for url in urls:\n",
    "            for source in sources:\n",
    "                if source in url.lower():\n",
    "                    urls_by_source[source].append({\n",
    "                        'url': url,\n",
    "                        'date': date,\n",
    "                        'tone': tone,\n",
    "                        'source': source\n",
    "                    })\n",
    "                    break\n",
    "    \n",
    "    print(f\"Debug - URLs extracted: {[(k, len(v)) for k, v in urls_by_source.items()]}\")\n",
    "    return urls_by_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77465175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_headline(url, max_retries=2):\n",
    "    \"\"\"Fetch headline from URL using newspaper3k.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            \n",
    "            if article.title:\n",
    "                return article.title\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed to fetch {url}: {e}\")\n",
    "                return None\n",
    "            time.sleep(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06726b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_articles_from_source(url_data_list, num_articles):\n",
    "    \"\"\"Sample random articles and fetch headlines.\"\"\"\n",
    "    if not url_data_list:\n",
    "        return []\n",
    "    \n",
    "    # Sample exactly num_articles (or all if fewer available)\n",
    "    sample_size = min(len(url_data_list), num_articles)\n",
    "    sampled = random.sample(url_data_list, sample_size)\n",
    "    \n",
    "    articles = []\n",
    "    \n",
    "    for url_data in sampled:\n",
    "        headline = fetch_headline(url_data['url'])\n",
    "        \n",
    "        if headline:\n",
    "            articles.append({\n",
    "                'Date': url_data['date'],\n",
    "                'Tone': url_data['tone'],\n",
    "                'Source': url_data['source'],\n",
    "                'Headline': headline,\n",
    "                'URL': url_data['url']\n",
    "            })\n",
    "            print(f\"✓ {url_data['source']}: {headline[:60]}...\")\n",
    "        else:\n",
    "            print(f\"✗ {url_data['source']}: Failed to fetch headline\")\n",
    "        \n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179ef033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(file_url, final_df):\n",
    "    \"\"\"Process a single GDELT file and append to final dataframe.\"\"\"\n",
    "    try:\n",
    "        # Download and parse\n",
    "        df = download_and_parse_gdelt(file_url)\n",
    "        if df is None or len(df) == 0:\n",
    "            print(\"No data in file\")\n",
    "            return final_df\n",
    "        \n",
    "        # Filter data\n",
    "        filtered_df = filter_gdelt_data(df, THEME, WESTERN_COUNTRY_CODES, MAJOR_SOURCES)\n",
    "        print(f\"Filtered to {len(filtered_df):,} relevant rows\")\n",
    "        \n",
    "        if len(filtered_df) == 0:\n",
    "            print(\"No relevant rows after filtering\")\n",
    "            return final_df\n",
    "        \n",
    "        # Extract URLs by source\n",
    "        print(\"Extracting URLs by source...\")\n",
    "        urls_by_source = extract_urls_by_source(filtered_df, MAJOR_SOURCES)\n",
    "        \n",
    "        # Determine minimum number of articles across sources\n",
    "        source_counts = {src: len(urls) for src, urls in urls_by_source.items() if len(urls) > 0}\n",
    "        if not source_counts:\n",
    "            print(\"No articles found for any source\")\n",
    "            return final_df\n",
    "        \n",
    "        min_count = min(min(source_counts.values()), ARTICLES_PER_SOURCE)\n",
    "        print(f\"\\nSampling {min_count} articles per source\")\n",
    "        print(f\"Available articles per source: {source_counts}\")\n",
    "        \n",
    "        # Sample articles from each source\n",
    "        all_articles = []\n",
    "        for source, url_data_list in urls_by_source.items():\n",
    "            if len(url_data_list) == 0:\n",
    "                print(f\"Skipping {source} - no articles available\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nProcessing {source}...\")\n",
    "            articles = sample_articles_from_source(url_data_list, min_count)\n",
    "            \n",
    "            if len(articles) == 0:\n",
    "                print(f\"Warning: Could not fetch any valid headlines from {source}\")\n",
    "            else:\n",
    "                all_articles.extend(articles)\n",
    "                print(f\"Successfully fetched {len(articles)} articles from {source}\")\n",
    "        \n",
    "        # Append to final dataframe\n",
    "        if all_articles:\n",
    "            new_df = pd.DataFrame(all_articles)\n",
    "            final_df = pd.concat([final_df, new_df], ignore_index=True)\n",
    "            print(f\"\\nAdded {len(all_articles)} articles. Total articles: {len(final_df)}\")\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_single_file (detailed): {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2431a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"Starting GDELT news scraper...\")\n",
    "    print(f\"Articles per source: {ARTICLES_PER_SOURCE}\\n\")\n",
    "    \n",
    "    # Initialize empty dataframe\n",
    "    final_df = pd.DataFrame(columns=['Date', 'Tone', 'Source', 'Headline', 'URL'])\n",
    "    \n",
    "    # Get list of all GDELT files\n",
    "    print(\"Fetching list of GDELT files...\")\n",
    "    file_list = get_gdelt_file_list()\n",
    "    \n",
    "    if not file_list:\n",
    "        print(\"No files found. Exiting.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(file_list)} GDELT files\\n\")\n",
    "    \n",
    "    # Process all files from target year\n",
    "    year = '2015'\n",
    "    files_year = [f for f in file_list if year in f]\n",
    "    print(f\"Found {len(files_year)} files from {year}\")\n",
    "\n",
    "    for i, file_url in enumerate(files_year, 1):\n",
    "        print(f\"Processing file {i}/{len(files_year)}\")\n",
    "        try:\n",
    "            final_df = process_single_file(file_url, final_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Add delay between files\n",
    "        if i < len(files_year):\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(f\"Scraping complete! Total articles collected: {len(final_df)}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    if len(final_df) > 0:\n",
    "        output_file = \"2015_political_articles.csv\"\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nSaved to {output_file}\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1138898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GDELT news scraper...\n",
      "Articles per source: 5\n",
      "\n",
      "Fetching list of GDELT files...\n",
      "Found 3947 GDELT files\n",
      "\n",
      "Found 365 files from 2015\n",
      "Processing file 1/365\n",
      "Downloading http://data.gdeltproject.org/gkg/20151231.gkg.csv.zip...\n",
      "Loaded 115,598 rows\n",
      "Filtered to 223 relevant rows\n",
      "Extracting URLs by source...\n",
      "Debug - URLs extracted: [('cnn.com', 53), ('theguardian.com', 80), ('washingtonexaminer.com', 36), ('newsweek.com', 4), ('breitbart.com', 50)]\n",
      "\n",
      "Sampling 4 articles per source\n",
      "Available articles per source: {'cnn.com': 53, 'theguardian.com': 80, 'washingtonexaminer.com': 36, 'newsweek.com': 4, 'breitbart.com': 50}\n",
      "\n",
      "Processing cnn.com...\n",
      "✓ cnn.com: Obama looks toward legacy in final year...\n",
      "Failed to fetch http://money.cnn.com/2015/12/31/pf/college/illinois-budget-college-grants/: Article `download()` failed with 502 Server Error: Bad Gateway for url: https://money.cnn.com/2015/12/31/pf/college/illinois-budget-college-grants/ on URL http://money.cnn.com/2015/12/31/pf/college/illinois-budget-college-grants/\n",
      "✗ cnn.com: Failed to fetch headline\n",
      "✓ cnn.com: Stop judging how Carrie Fisher is aging...\n",
      "Failed to fetch http://money.cnn.com/2015/12/17/pf/new-year-financial-resolutions-2016/?sr=recirc123115moneyresolutions930story: Article `download()` failed with 502 Server Error: Bad Gateway for url: https://money.cnn.com/2015/12/17/pf/new-year-financial-resolutions-2016/?sr=recirc123115moneyresolutions930story on URL http://money.cnn.com/2015/12/17/pf/new-year-financial-resolutions-2016/?sr=recirc123115moneyresolutions930story\n",
      "✗ cnn.com: Failed to fetch headline\n",
      "Successfully fetched 2 articles from cnn.com\n",
      "\n",
      "Processing theguardian.com...\n",
      "✓ theguardian.com: Poland’s new leader seems hungry for total control. The west...\n",
      "✓ theguardian.com: Canada misses target of 10,000 Syrian refugees by year's end...\n",
      "✓ theguardian.com: Bob Hawke: Obama 'inadequate' in resolving Israeli-Palestini...\n",
      "✓ theguardian.com: San Bernardino shooters' neighbour Enrique Marquez indicted ...\n",
      "Successfully fetched 4 articles from theguardian.com\n",
      "\n",
      "Processing washingtonexaminer.com...\n",
      "✗ washingtonexaminer.com: Failed to fetch headline\n",
      "✗ washingtonexaminer.com: Failed to fetch headline\n",
      "✓ washingtonexaminer.com: Chinese spy balloon initial report not marked urgent, per of...\n",
      "✓ washingtonexaminer.com: Can an independent mechanic be a cybersecurity risk?...\n",
      "Successfully fetched 2 articles from washingtonexaminer.com\n",
      "\n",
      "Processing newsweek.com...\n",
      "Failed to fetch http://europe.newsweek.com/big-shots-weeks-news-photographs-408584: Article `download()` failed with 403 Client Error: Forbidden for url: http://europe.newsweek.com/big-shots-weeks-news-photographs-408584 on URL http://europe.newsweek.com/big-shots-weeks-news-photographs-408584\n",
      "✗ newsweek.com: Failed to fetch headline\n",
      "Failed to fetch http://europe.newsweek.com/should-we-just-read-books-instead-410171: Article `download()` failed with 403 Client Error: Forbidden for url: http://europe.newsweek.com/should-we-just-read-books-instead-410171 on URL http://europe.newsweek.com/should-we-just-read-books-instead-410171\n",
      "✗ newsweek.com: Failed to fetch headline\n",
      "Failed to fetch http://europe.newsweek.com/central-african-republic-car-seleka-anti-balaka-410041: Article `download()` failed with 403 Client Error: Forbidden for url: http://europe.newsweek.com/central-african-republic-car-seleka-anti-balaka-410041 on URL http://europe.newsweek.com/central-african-republic-car-seleka-anti-balaka-410041\n",
      "✗ newsweek.com: Failed to fetch headline\n",
      "Failed to fetch http://europe.newsweek.com/john-mcafee-running-president-speaks-newsweek-410536: Article `download()` failed with 403 Client Error: Forbidden for url: http://europe.newsweek.com/john-mcafee-running-president-speaks-newsweek-410536 on URL http://europe.newsweek.com/john-mcafee-running-president-speaks-newsweek-410536\n",
      "✗ newsweek.com: Failed to fetch headline\n",
      "Warning: Could not fetch any valid headlines from newsweek.com\n",
      "\n",
      "Processing breitbart.com...\n",
      "Failed to fetch http://wbay.com/ap/after-daunting-2015-eu-faces-year-of-living-dangerously/<UDIV>http://wjtv.com/ap/after-daunting-2015-eu-faces-year-of-living-dangerously/<UDIV>http://www.breitbart.com/news/after-daunting-2015-eu-faces-year-of-living-dangerously/: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wbay.com/ap/after-daunting-2015-eu-faces-year-of-living-dangerously/%3CUDIV%3Ehttp:/wjtv.com/ap/after-daunting-2015-eu-faces-year-of-living-dangerously/%3CUDIV%3Ehttp:/www.breitbart.com/news/after-daunting-2015-eu-faces-year-of-living-dangerously/ on URL http://wbay.com/ap/after-daunting-2015-eu-faces-year-of-living-dangerously/<UDIV>http://wjtv.com/ap/after-daunting-2015-eu-faces-year-of-living-dangerously/<UDIV>http://www.breitbart.com/news/after-daunting-2015-eu-faces-year-of-living-dangerously/\n",
      "✗ breitbart.com: Failed to fetch headline\n",
      "✓ breitbart.com: Israel Cracks Down on Islamic State Volunteers...\n",
      "✓ breitbart.com: Israel Plays Down U.S. Spying Scandal...\n",
      "✓ breitbart.com: Rap Sheet: The Women Who Claim to Be Victims of Bill and Hil...\n",
      "Successfully fetched 3 articles from breitbart.com\n",
      "\n",
      "Added 11 articles. Total articles: 11\n",
      "Processing file 2/365\n",
      "Downloading http://data.gdeltproject.org/gkg/20151230.gkg.csv.zip...\n",
      "Loaded 124,054 rows\n",
      "Filtered to 251 relevant rows\n",
      "Extracting URLs by source...\n",
      "Debug - URLs extracted: [('cnn.com', 49), ('theguardian.com', 89), ('washingtonexaminer.com', 46), ('newsweek.com', 6), ('breitbart.com', 61)]\n",
      "\n",
      "Sampling 5 articles per source\n",
      "Available articles per source: {'cnn.com': 49, 'theguardian.com': 89, 'washingtonexaminer.com': 46, 'newsweek.com': 6, 'breitbart.com': 61}\n",
      "\n",
      "Processing cnn.com...\n",
      "✓ cnn.com: George Pataki drops presidential bid...\n",
      "Failed to fetch http://money.cnn.com/2015/12/30/investing/stocks-2016-europe/: Article `download()` failed with 502 Server Error: Bad Gateway for url: https://money.cnn.com/2015/12/30/investing/stocks-2016-europe/ on URL http://money.cnn.com/2015/12/30/investing/stocks-2016-europe/\n",
      "✗ cnn.com: Failed to fetch headline\n",
      "✓ cnn.com: Coincidences: What causes them?...\n",
      "Failed to fetch http://edition.cnn.com/2015/12/30/entertainment/carrie-fisher-star-wars-aging-response-twitter/<UDIV>http://www.cnn.com/2015/12/30/entertainment/carrie-fisher-star-wars-aging-response-twitter/: Article `download()` failed with 404 Client Error: Not Found for url: http://edition.cnn.com/2015/12/30/entertainment/carrie-fisher-star-wars-aging-response-twitter/%3CUDIV%3Ehttp://www.cnn.com/2015/12/30/entertainment/carrie-fisher-star-wars-aging-response-twitter/ on URL http://edition.cnn.com/2015/12/30/entertainment/carrie-fisher-star-wars-aging-response-twitter/<UDIV>http://www.cnn.com/2015/12/30/entertainment/carrie-fisher-star-wars-aging-response-twitter/\n",
      "✗ cnn.com: Failed to fetch headline\n",
      "✓ cnn.com: Why art auction records keep being broken...\n",
      "Successfully fetched 3 articles from cnn.com\n",
      "\n",
      "Processing theguardian.com...\n",
      "✓ theguardian.com: Thatcher tried to block 'bad taste' public health warnings a...\n",
      "✓ theguardian.com: The truth still hurts: the enduring gallows humour of Hangme...\n",
      "✓ theguardian.com: Oxford university donations that still court controversy | L...\n",
      "✓ theguardian.com: Drug company taken over by Martin Shkreli seeks bankruptcy p...\n",
      "✓ theguardian.com: Lawmakers only care about others' privacy when their own is ...\n",
      "Successfully fetched 5 articles from theguardian.com\n",
      "\n",
      "Processing washingtonexaminer.com...\n",
      "✓ washingtonexaminer.com: Mystery wrapped in a spy balloon: Why did China blow up Blin...\n",
      "✓ washingtonexaminer.com: Nothing says ‘harm reduction’ like helping addicts overdose...\n",
      "✗ washingtonexaminer.com: Failed to fetch headline\n",
      "✗ washingtonexaminer.com: Failed to fetch headline\n",
      "✓ washingtonexaminer.com: Zelensky declares victory after leaving NATO summit with new...\n",
      "Successfully fetched 3 articles from washingtonexaminer.com\n",
      "\n",
      "Processing newsweek.com...\n",
      "Failed to fetch http://europe.newsweek.com/syrian-troops-backed-jets-enters-rebel-held-town-410147: Article `download()` failed with 403 Client Error: Forbidden for url: http://europe.newsweek.com/syrian-troops-backed-jets-enters-rebel-held-town-410147 on URL http://europe.newsweek.com/syrian-troops-backed-jets-enters-rebel-held-town-410147\n",
      "✗ newsweek.com: Failed to fetch headline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch http://europe.newsweek.com/us-preparing-fresh-sanctions-iran-wsj-410300: Article `download()` failed with 403 Client Error: Forbidden for url: http://europe.newsweek.com/us-preparing-fresh-sanctions-iran-wsj-410300 on URL http://europe.newsweek.com/us-preparing-fresh-sanctions-iran-wsj-410300\n",
      "✗ newsweek.com: Failed to fetch headline\n",
      "Failed to fetch http://europe.newsweek.com/real-africa-through-lens-african-photographers-407886: Article `download()` failed with 403 Client Error: Forbidden for url: http://europe.newsweek.com/real-africa-through-lens-african-photographers-407886 on URL http://europe.newsweek.com/real-africa-through-lens-african-photographers-407886\n",
      "✗ newsweek.com: Failed to fetch headline\n",
      "Failed to fetch http://europe.newsweek.com/how-stop-europes-jewish-exodus-israel-408822: Article `download()` failed with 403 Client Error: Forbidden for url: http://europe.newsweek.com/how-stop-europes-jewish-exodus-israel-408822 on URL http://europe.newsweek.com/how-stop-europes-jewish-exodus-israel-408822\n",
      "✗ newsweek.com: Failed to fetch headline\n",
      "Failed to fetch http://europe.newsweek.com/china-taiwan-open-first-hotline-reduce-tensions-409852: Article `download()` failed with 403 Client Error: Forbidden for url: http://europe.newsweek.com/china-taiwan-open-first-hotline-reduce-tensions-409852 on URL http://europe.newsweek.com/china-taiwan-open-first-hotline-reduce-tensions-409852\n",
      "✗ newsweek.com: Failed to fetch headline\n",
      "Warning: Could not fetch any valid headlines from newsweek.com\n",
      "\n",
      "Processing breitbart.com...\n",
      "✓ breitbart.com: Egyptian Political Scientist: Country Must Acknowledge ‘Veng...\n",
      "✓ breitbart.com: Charles Hurt: Mainstream Media Underwent ’12 Stages of Denia...\n",
      "✓ breitbart.com: CNC ‘Ghost Gunner’ Machine Forges Path for Homemade AR-15 Ri...\n",
      "✓ breitbart.com: Marco Rubio On Fox News: Let’s Make America Great Again!...\n",
      "✓ breitbart.com: Miami Reemerging as Trafficking Hub for Colombian Cocaine...\n",
      "Successfully fetched 5 articles from breitbart.com\n",
      "\n",
      "Added 16 articles. Total articles: 27\n",
      "Processing file 3/365\n",
      "Downloading http://data.gdeltproject.org/gkg/20151229.gkg.csv.zip...\n",
      "Loaded 117,137 rows\n",
      "Filtered to 210 relevant rows\n",
      "Extracting URLs by source...\n",
      "Debug - URLs extracted: [('cnn.com', 54), ('theguardian.com', 72), ('washingtonexaminer.com', 36), ('newsweek.com', 0), ('breitbart.com', 48)]\n",
      "\n",
      "Sampling 5 articles per source\n",
      "Available articles per source: {'cnn.com': 54, 'theguardian.com': 72, 'washingtonexaminer.com': 36, 'breitbart.com': 48}\n",
      "\n",
      "Processing cnn.com...\n",
      "Failed to fetch http://money.cnn.com/2015/12/28/media/donald-trump-mcquaid-response/: Article `download()` failed with 502 Server Error: Bad Gateway for url: https://money.cnn.com/2015/12/28/media/donald-trump-mcquaid-response/ on URL http://money.cnn.com/2015/12/28/media/donald-trump-mcquaid-response/\n",
      "✗ cnn.com: Failed to fetch headline\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mfetch_headline\u001b[0;34m(url, max_retries)\u001b[0m\n\u001b[1;32m      6\u001b[0m article\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[0;32m----> 7\u001b[0m article\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m article\u001b[38;5;241m.\u001b[39mtitle:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/newspaper/article.py:191\u001b[0m, in \u001b[0;36mArticle.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthrow_if_not_downloaded_verbose()\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_parser()\u001b[38;5;241m.\u001b[39mfromstring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/newspaper/article.py:531\u001b[0m, in \u001b[0;36mArticle.throw_if_not_downloaded_verbose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_state \u001b[38;5;241m==\u001b[39m ArticleDownloadState\u001b[38;5;241m.\u001b[39mFAILED_RESPONSE:\n\u001b[0;32m--> 531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle `download()` failed with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on URL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    532\u001b[0m           (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_exception_msg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl))\n",
      "\u001b[0;31mArticleException\u001b[0m: Article `download()` failed with 502 Server Error: Bad Gateway for url: https://money.cnn.com/news/newsfeeds/articles/prnewswire/LA88380.htm on URL http://money.cnn.com/news/newsfeeds/articles/prnewswire/LA88380.htm",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_df \u001b[38;5;241m=\u001b[39m main()\n",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files_year)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     final_df \u001b[38;5;241m=\u001b[39m process_single_file(file_url, final_df)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m, in \u001b[0;36mprocess_single_file\u001b[0;34m(file_url, final_df)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m articles \u001b[38;5;241m=\u001b[39m sample_articles_from_source(url_data_list, min_count)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Could not fetch any valid headlines from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36msample_articles_from_source\u001b[0;34m(url_data_list, num_articles)\u001b[0m\n\u001b[1;32m     10\u001b[0m articles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url_data \u001b[38;5;129;01min\u001b[39;00m sampled:\n\u001b[0;32m---> 13\u001b[0m     headline \u001b[38;5;241m=\u001b[39m fetch_headline(url_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m headline:\n\u001b[1;32m     16\u001b[0m         articles\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m: url_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTone\u001b[39m\u001b[38;5;124m'\u001b[39m: url_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtone\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m: url_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m         })\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mfetch_headline\u001b[0;34m(url, max_retries)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
